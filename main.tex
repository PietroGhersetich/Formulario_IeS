% Geometria documento.
\documentclass[8pt]{extarticle}
\usepackage[a4paper, landscape, left=0.25cm, right=0.25cm, top=0.25cm, bottom=0.25cm]{geometry}
% Allineamento contenuti.
\usepackage[document]{ragged2e}
\usepackage{multicol}
% Rimozione spazi verticali liste.
\usepackage{enumitem}
\setlist[itemize]{nosep}
% Utility per formule.
\usepackage{mathtools, nccmath}
\usepackage{amssymb, amsmath}


\begin{document}
% Rimuovo la numerazione delle pagine.
\thispagestyle{empty}
% Ambiente multicolonna su tutto il documento.
\begin{multicols*}{3}

    \textbf{\underline{Probabilità}}
    \begin{itemize}
        \item \textbf{Teo. Prob. Totali}: $P(\bigcup_{i=1}^{n}P(A_{i}))=\sum_{i=1}^{n}P(A_{i})$; $A_{1},...,A_{n}$ eventi indipendenti.
        \item \textbf{Prob. Condizionate}: $P(A|B)={P(A\cap B)}/{P(B)}$.
        \item \textbf{Teo. Prob. Totali (Partizioni)}: $P(A)=\sum_{i=1}^{n}P(B_{i})P(A|B_{i})$, $B_{1},...,B_{n}$ partizioni di $\Omega$.
        \item \textbf{Teo. Bayes}: $P(A_{i}|B)=P(B|A_{i})P(A_{i})/(\sum_{j=1}^{n}P(B|A_{j})P(A_j)=P(B))$, $A_{i}\in A_{1},...,A_{n}$ partizioni di $\Omega$.
        \item \textbf{Note sui Complementari}: $P(A)=1-P(A^{C})$; $P(A|B)=1-P(A^{C}|B)$.
        \item \textbf{Indipendenza}: $A_{1},...,A_{n}$ eventi indipendenti sse $P(\bigcap_{i=1}^{n}P(A_{i}))=\prod_{i=1}^{n}P(A_{i})$.
        \item \textbf{Note sull'Indipendenza}: $P(A|B)=P(A)$ sse $A, B$ eventi indipendenti; indipendenza condizionata $\nleftrightarrow$ indipendenza incondizionata.
    \end{itemize}

    \textbf{\underline{Calcolo Combinatorio}}
    \begin{itemize}
        \item \textbf{Permutazioni}: $n$ elementi distinti $\rightarrow$ $n!$ permutazioni; se $m\leq n$ elementi sono indistinguibili $\rightarrow$ ${n!}/{m!}$ disposizioni.
        \item \textbf{Sottoinsiemi di $k$ elementi}: $n$ elementi, assunto $k\leq n$ $\rightarrow$ ${n!}/{(n-k)!}$ sottoinsiemi; se l'ordine degli elementi all'interno dei sottoinsiemi non mi importa avrò $\binom{n}{k}={n!}/{k!(n-k)!}$ sottoinsiemi.
        \item \textbf{Prob. Binomiale}: dati $n$ tentativi, $P(succ.)=p$, assunto $k\leq n$ $\rightarrow$ $P({k\; succ.}/{n\; tentativi})=\binom{n}{k}p^{k}(1-p)^{n-k}$.
        \item \textbf{Partizioni}: dato $\Omega$ partizionato in $K_{n}$ sottoinsiemi, calcolo la probabilità, su $n$ tentativi, di ottenere $k_{1}\in K_{1},...,k_{n}\in K_{n}$ elementi $\rightarrow$ $\binom{n}{k_{1},...,k_{n}}={n!}/{k_{1}!k_{2}!...k_{n}}$.
        \item \textbf{Prob. Ipergeometrica}: dato $|\Omega|=n$ partizionato in $K_{1}=k\; K_{2}=n-k$ sottoinsiemi e scelto un campione di $c<n$ elementi voglio calcolare la probabilità che questo sia composto da $k^{'}\leq k\in K_{1}$ e $k^{''} = c-k^{'}\leq n-k\in K_{2}$ elementi $\rightarrow$ ${\binom{k}{k^{'}}\binom{n-k}{c-k^{'}}}/{\binom{n}{c}}$.
    \end{itemize}

    \textbf{\underline{Variabili Aleatorie Discrete}}
    \begin{itemize}
        \item \textbf{Legge di Prob.}: $p_{X}(x)=P(X=x)$.
        \item \textbf{Valore Atteso}: $E[X]=\sum_{x}x\cdot p_{X}(x)$; $E[X|A]=\sum_{x}x\cdot p_{X|A}(x)$ se condizionato.
        \item \textbf{Legge dello Statistico Inconsapevole}: sia $g(x)$ deterministica $\rightarrow$ $E[g(X)]=\sum_{x}g(x)\cdot p_{X}(x)$.
        \item \textbf{Linearità del Valore Atteso}: $E[\alpha X+\beta]=\alpha E[X]+\beta$; se $g(x)$ è determinista e lineare $E[g(X)]=g(E[X])$.
        \item \textbf{Varianza}: $Var[X]=E[X^{2}]-E[X]^{2}=E[(X-E[X])^{2}]$.
        \item \textbf{Semi-Linearità della Varianza}: $Var[\alpha X+\beta]=\alpha^{2}Var[X]$.
        \item \textbf{Deviazione Standard}: $\sigma_{X}=\sqrt{Var[X]}$.
        \item \textbf{Perdita di Memoria}: $p_{X-t|X>t}(x)=p_{X}(x)$.
        \item \textbf{Legge dell'Aspettativa Totale}: $E[X]=\sum_{i=1}^{n}P(A_{i})E(X|A_{i})$, $A_{1},...,A_{n}$ eventi che partizionano $\Omega$.
    \end{itemize}

    \textbf{\underline{Variabili Aleatorie Discrete Multiple}}
    \begin{itemize}
        \item \textbf{Marginalizzazione}: $p_{Y}(y)=\sum_{x}p_{X,Y}(x,y)=\sum_{x}p_{X}(x)\cdot p_{Y|X}(y|x)$.
        \item \textbf{Valore Atteso}: data $g(x,y)$ deterministica $E[g(X,Y)]=\sum_{x}\sum_{y}g(x,y)\cdot p_{X,Y}(x,y)$; caso particolare $E[X+Y]=E[X]+E[Y]$.
        \item \textbf{Varianza}: $Var[X+Y]=Var[X]+Var[Y]+2(E[XY]-E[X]E[Y])$.
        \item \textbf{Casi Particolari (X, Y Indipendenti)}:
              \useshortskip \begin{equation*}
                  X\perp Y\Rightarrow \begin{cases}E[XY]=E[X]E[Y]\\ E[g(X)h(Y)]=E[g(X)]E[h(Y)]\\Var[\alpha X+\beta Y]=\alpha^{2}Var[X]+\beta^{2}Var[Y]\end{cases}
              \end{equation*}
    \end{itemize}

    \textbf{\underline{Variabili Aleatorie Continue}}
    \begin{itemize}
        \item \textbf{Densità di Prob.}: $P(x\leq X\leq x+\delta)=\int_{x}^{x+\delta}f_{X}(\gamma)d\gamma$.
        \item \textbf{Valore Atteso}: $E[X]=\int_{-\infty}^{\infty}xf_{X}(x)dx$.
        \item \textbf{Legge dello Statistico Inconsapevole}: $E[g(X)]=\int_{-\infty}^{\infty}g(x)f_{X}(x)dx$.
        \item \textbf{Varianza}: $\sigma^{2}=Var[X]=E[X^{2}]-E[X]^{2}=\int_{-\infty}^{\infty}(x-E[X])^{2}\cdot f_{X}(x)dx$.
        \item \textbf{Cumulata di Prob.}: $F_{X}(x)=P(X\leq x)=\int_{-\infty}^{x}f_{X}(\gamma)d\gamma$.
    \end{itemize}

    \textbf{\underline{Variabili Aleatorie Continue Multiple}}
    \begin{itemize}
        \item \textbf{Marginalizzazione}: $f_{Y}(y)=\int_{x}f_{X,Y}(x,y)dx=\int_{y}f_{Y|X}(y|x)f_{X}(x)dx$.
        \item \textbf{Valore Atteso}: data $g(x,y)$ deterministica $E[g(X,Y)]=\iint_{\mathbb{R}}g(X,Y)f_{X,Y}(x,y)dxdy$.
        \item \textbf{Casi Particolari (X, Y Indipendenti)}: $X\perp Y\Rightarrow f_{X,Y}(x,y)=f_{X}(x)f_{Y}(y)\wedge F_{X,Y}(x,y)=F_{X}(x)F_{Y}(y)$.
        \item \textbf{Teo. Bayes nel Continuo e Situazioni Ibride}:
              \begin{itemize}
                  \item $X,Y$ continue: $f_{X|Y}(x,y)=f_{Y|X}(y|x)f_{X}(x)/f_{Y}(y)$.
                  \item $X$ discreta, $Y$ continua: $p_{X|Y}(x|y)=f_{Y|X}(y|x)p_{X}(x)/f_{Y}(y)$.
                  \item $X$ continua, $Y$ discreta: $f_{X|Y}(x|y)=p_{Y|X}(y|x)f_{X}(x)/p_{Y}(y)$.
              \end{itemize}
        \item \textbf{Somma di Variabili Aleatorie Continue}: Sia $Z=X+Y$:
              \begin{itemize}
                  \item $X\perp Y\Longleftrightarrow f_{Z,X}(z,x)=f_X(x)f_Y(z-x)$.
                  \item $f_Z(z)=\int_{-\infty}^{+\infty}f_X(x)f_Y(z-x)dx$. \textbf{Convoluzione.}
                  \item $F_Z(z)=F_{X+Y}(z)=\int_{-\infty}^{+\infty}F_x(z-y)f_Y(y)dy$.
              \end{itemize}
    \end{itemize}

    \textbf{\underline{Calcolo di una Funzione di V.A.}}
    \begin{itemize}
        \item \textbf{Discr.} $p_{Y}(y)=P(g(X)=y)=\sum_{x|g(x)=y}p_{X}(x)$.
        \item \textbf{Cont. (Cumulata)} $F_{Y}(y)=P(g(X)\leq y)\implies f_{Y}(y)=dF_{y}(y)/dy$.
        \item \textbf{Cont. ($g(X)$ Lineare)} $Y=\alpha X+\beta\implies f_{Y}(y)=\frac{1}{|\alpha|}f_{X}(\frac{x-\beta}{\alpha})$.
        \item \textbf{Cont. ($g(X)$ Monotona)} $Y=g(X)\implies f_{Y}(y)=f_{X}(g^{-1}(y))/\left|\frac{d}{dx}g(g^{-1}(y))\right|$.
    \end{itemize}

    \textbf{\underline{Altri Indicatori Statistici per V.A. Multiple}}
    \begin{itemize}
        \item \textbf{Covarianza}
              \begin{itemize}
                  \item $Cov[X,Y]=E[XY]-E[X]E[Y]$.
                  \item $Cov[X,X]=Var[X]$; $Cov[\alpha X,Y]=\alpha Cov[X,Y]$.
                  \item $X\perp Y\Rightarrow Cov[X,Y]=0$ ma $Cov[X,Y]=0 \nRightarrow X\perp Y$.
                  \item $Cov[\sum_{i=1}^{n}X_{i},\sum_{j=1}^{n}Y_{j}]=\sum_{i=1}^{n}\sum_{j=1}^{n}Cov[X_{i},Y_{j}]$.
                  \item $Var[\sum_{i=1}^{n}X_{i}]=\sum_{i=1}^{n}Var[X_{i}]+2\sum \sum_{i<j}Cov[X_{i},X_{j}]$.
              \end{itemize}
        \item \textbf{Coeff. Correlazione Lineare}: $\rho =Cov[X,Y]/\sigma_{X}\sigma_{Y}$. Se $\rho=1\implies (X-E[X])=\alpha(Y-E[Y])$.
        \item \textbf{Valore Atteso Condizionato}: $E[X|Y]=\sum_{x}x\cdot p_{X|Y}(x|y)$.
        \item \textbf{Legge delle Aspettazioni Iterate}: $E[X]=E[E[X|Y]]$.
        \item \textbf{Varianza Condizionata}: $Var[X|Y]=E[X^{2}|Y=y]-E[X|Y=y]^{2}$.
        \item \textbf{Legge della Variazione Totale}: $Var[X]=E[Var[X|Y]]+Var[E[X|Y]]$.
    \end{itemize}

    \textbf{\underline{Successioni di V.A.}}
    \begin{itemize}
        \item \textbf{Somma di $N$ (casuale) V.A. Indipendenti $X_{i}$}:
              \begin{itemize}
                  \item $E[\sum_{i=1}^{N}X_{i}]=E[E[\sum_{i=1}^{N}X_{i}N]]=E[N]E[X]$.
                  \item $Var[X]=E[N]Var[X_{1}]+Var[N]E[X_{1}]^{2}$.
              \end{itemize}
        \item \textbf{Diseguaglianza di Markov}: $E[X]\geq \alpha P(X\geq \alpha)$.
        \item \textbf{Diseguaglianza di Chebyshev}: $Var[X]\geq \alpha^{2} P(|X-E[X]|\geq \alpha)$.
        \item \textbf{Convergenza in Prob.}: Sia ${A_{k}}$ una successione di V.A. e sia $\alpha\in\mathbb{R}$; ${A_{k}}$ si dice convergente in probabilità ad $\alpha$ se: $\lim_{k\rightarrow +\infty}P(|A_{k}-a|\geq\epsilon)=0\;\forall\epsilon >0$. $A_{k}\rightarrow^{P}\alpha$.
        \item \textbf{Media Campionaria}: Siano $X_{1},X_{2},..,X_{n}$ V.A. I.I.D.; $M_{n}=(X_{1}+X_{2}+..+X_{n})/n$. $M_{n}\rightarrow^{P} E[X]$; $E[M_{n}]=^{n\rightarrow +\infty}E[X]$; $Var[M_{n}]=^{n\rightarrow +\infty}0$.
    \end{itemize}

    \textbf{\underline{V.A. Notevoli}}
    \begin{itemize}
        \item \textbf{V.A. Uniforme $X\sim\mathcal{U}[\alpha, \beta]$} (cont.):
              \begin{itemize}
                  \item "Attribuisce la stessa probabilità ad ogni elemento dell'insieme $S$ su cui è definita".
                  \item $f_{X}(x)=\begin{cases}
                                1/(\beta-\alpha) & \alpha<x<\beta \\
                                0                & altr.
                            \end{cases}$
                  \item $F_{X}(x)=\begin{cases}
                                0                         & x<\alpha       \\
                                (x-\alpha)/(\beta-\alpha) & \alpha<x<\beta \\
                                1                         & x>\beta
                            \end{cases}$
                  \item $E[X]=\frac{1}{2}(\alpha +\beta)$; $Var[X]=\frac{1}{12}(\beta -\alpha)^{2}$.
              \end{itemize}
        \item \textbf{V.A. Uniforme $X\sim\mathcal{U}\{\alpha, \beta\}$} (discr.):
              \begin{itemize}
                  \item $p_{X}(x)=1/n$.
                  \item $E[X]=\frac{1}{2}(\alpha +\beta)$; $Var[X]=\frac{1}{12}(n^{2}-1)$.
              \end{itemize}
        \item \textbf{V.A. Geometrica $X\sim\mathcal{G}(p)$} (discr.):
              \begin{itemize}
                  \item "Probabilità che il primo successo (o evento in generale) richieda l'esecuzione di $x$ prove indipendenti, ognuna con probabilità di successo $p$".
                  \item $p_{X}(x)=(1-p)^{x-1}p$.
                  \item $E[X]=1/p$; $Var[X]=(1-p)/p^{2}$.
              \end{itemize}
        \item \textbf{V.A. Binomiale/di Bernoulli $X\sim\mathcal{B}(n,p)$ (se $n=1$ Binomiale $\rightarrow$ Bernulliana)} (discr.):
              \begin{itemize}
                  \item "Probabilità di ottenere $i$ successi in $n$ prove indipendenti, ognuna con probabilità di successo $p$".
                  \item $p_{X_{i}}(1/succ.)=p$; $p_{X_{i}}(0/insucc.)=1-p$.
                  \item $p_{B}(i)=p_{X_{1}+X_{2}+...+X_{n}}(i)=\binom{n}{i}p^{i}(1-p)^{n-i}$.
                  \item $E[X]=np$; $Var[X]=np(1-p)$.
              \end{itemize}
        \item \textbf{V.A. Esponenziale $X\sim\mathcal{E}(\lambda)$} (cont.).
              \begin{itemize}
                  \item $f_{X}(x)=\begin{cases}
                                \lambda e^{-x\lambda} & x\geq 0 \\
                                0                     & x<0
                            \end{cases}$.
                  \item $F_{X}(x)=1-e^{-x\lambda}$.
                  \item $E[X]=1/\lambda$; $Var[X]=1/\lambda^{2}$.
              \end{itemize}
        \item \textbf{V.A. Gaussiana $X\sim\mathcal{N}(\mu,\sigma^{2})$} (cont.).
              \begin{itemize}
                  \item $f_{X}(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}$.
                  \item $E[X]=\mu$; $Var[X]=\sigma^{2}$.
                  \item $\mathcal{N}(0,1)$ è la gaussiana standard i cui valori di cumulata sono tabulati; $X\sim\mathcal{N}(\mu, \sigma^{2})\implies \frac{X-\mu}{\sigma}\sim\mathcal{N}(0,1)$.
                  \item $Y=\alpha X+\beta\implies Y\sim\mathcal{N}(\alpha\mu +\beta,\alpha^{2}\sigma^{2})$.
                  \item $X\perp Y$; $X\sim\mathcal{N}(\mu_{'}, \sigma_{'}^{2})$, $Y\sim\mathcal{N}(\mu_{''}, \sigma_{''}^{2})$; $Z=X+Y\sim\mathcal{N}(\mu_{'}+\mu_{''}, \sigma_{'}^{2}+\sigma_{''}^{2})$. In ogni caso, anche se $X\not\perp Y$, $E[Z]=E[X]+E[Y]$; la varianza, invece, necessita del fattore correttivo. La somma di gaussiane sarà sempre gaussiana.
              \end{itemize}
    \end{itemize}

    \textbf{\underline{Teorema fondamentale del limite}}
    \begin{itemize}
        \item \textbf{C.L.T.}: Siano $X_{i}$ v.a. i.i.d. con $Var[X_{i}]=\sigma^{2}$ finita e $E[X_{i}]=\mu$. Allora, $S_{n}=X_{1}+X_{2}+...+X_{n}$ è gaussiana per $n\rightarrow\infty$; normalizzando: $Z_{n}=\frac{S_{n}-E[S_{n}]}{\sqrt{Var[S_{n}]}}=\frac{S_{n}-n\mu}{\sigma\sqrt{n}}\sim\mathcal{N}(0,1)$.
        \item \textbf{Problema del sondaggista}: Sia $M_{n}$ una media campionaria di v.a. i.i.d. $X_{i}$ con $E[X_{i}]=\mu$ e $Var[X_{i}]=\sigma^{2}$. Allora, per $n\rightarrow\infty$, $P(|S_{n}-E[S_{n}]|<\alpha)\ge\beta$ è calcolabile come: $P\left(Z<\frac{\alpha\sqrt{n}}{\sigma}\right)\ge\beta$ dove $Z\sim\mathcal{N}(0,1)$.
        \item \textbf{Th. De Moivre-Laplace}: Sia $X\sim\mathcal{B}(n,p)$, allora $Z_{n}=\frac{X-np}{\sqrt{np(1-p)}}\sim\mathcal{N}(0,1)$ per $n\rightarrow\infty$. Inoltre, $P(a<X<b)\approx\Phi\left(\frac{b-np}{\sqrt{np(1-p)}}\right)-\Phi\left(\frac{a-np}{\sqrt{np(1-p)}}\right)$.
    \end{itemize}

    \textbf{\underline{Processi casuali}}
    \begin{itemize}
        \item \textbf{Proc. di Bernoulli}: $BP(p)$ è una serie di v.a. i.i.d. $X_{i}\sim Bern(p)$.
              \begin{itemize}
                  \item \textit{Numero di successi (arrivi) $S$ in $n$ istanti temporali}: $S\sim\mathcal{B}(n,p)$.
                  \item \textit{Tempo di interarrivo}: $T_{i}\sim\mathcal{G}(p)$. Tutti i tempi di interarrivo sono indipendenti e godono di perdita di memoria.
                  \item \textit{Tempo al $k$-esimo arrivo}: $Y_{k}=T_{1}+T_{2}+...+T_{k}$. $Y_{k}\sim Pascal(p)$ v.a. di Pascal.
                        \begin{itemize}
                            \item $P(Y_{k}=t)=P(k-1\; arr. in\; [1, t-1], arr.\; in\; t)=p\binom{t-1}{k-1}p^{k-1}(1-p)^{t-k}$ assunto $t\ge k\ge 1$.
                            \item $E[Y_{k}]=k/p$.
                            \item $Var[Y_{k}]=k(1-p)/p^{2}$.
                        \end{itemize}
                  \item \textit{Splitting di un B.P.}: gli arrivi di $BP(p)$ possono essere "divisi" in due sottoprocessi $BP(pq)$ e $BP(p(1-q))$ con $q$ prob., per un arrivo, di finire nel primo e $1-q$ di finire nel secondo b.p.; i due processi sono indipendenti.
                  \item \textit{Merging di due B.P.}: gli arrivi di $BP(p)$ e $BP(q)$ possono essere riuniti in un unico $BP(p+q-pq)$.
              \end{itemize}
        \item \textbf{Proc. di Poisson}: $PP(\lambda)$ è la versione continua dei B.P.
              \begin{itemize}
                  \item \textit{Probabilità di avere $k$ arrivi nell'intervallo $[0, \tau]$}:  $N[0, \tau]\sim Poisson(\lambda\tau)$ è una v.a. di Poisson.
                        \begin{itemize}
                            \item $P(N[0, \tau]=k)=\begin{cases}
                                          \frac{(\lambda\tau)^{k}}{k!}e^{-\lambda\tau} & k\in\mathbb{N} \\
                                          0                                            & altr.
                                      \end{cases}$.
                            \item $E[N[0, \tau]]=Var[N[0, \tau]]=\lambda\tau$.
                            \item $\sum_{k=0}^{+\infty}P_{N[0,\tau]}(k)=1$.
                        \end{itemize}
                  \item \textit{Distribuzione del tempo al $k$-esimo arrivo}: $Y_{k}\sim Erlang-k(\lambda)$.
                        \begin{itemize}
                            \item $f_{Y_{k}}(t)=\begin{cases}
                                          \frac{(\lambda t)^{k-1}}{(k-1)!}e^{-\lambda t}\lambda & t>0, k\ge 1 \\
                                          0                                                     & altr.
                                      \end{cases}$
                            \item $E[Y_{k}]=\frac{k}{\lambda}$, $Var[Y_{k}]=\frac{k}{\lambda^{2}}$.
                        \end{itemize}
                  \item \textit{Splitting di un P.P.}: $PP(\lambda)\rightarrow PP(\lambda q)\wedge PP(\lambda(1-q))$.
                  \item \textit{Merging di due P.P.}: $PP(\lambda_{1})\wedge PP(\lambda_{2})\rightarrow PP(\lambda_{1}+\lambda_{2})$.
                  \item \textit{Incidenza casuale per P.P.} Dato un P.P. iniziato da un tempo indefinito, il tempo tra due arrivi una $Erlang-2(\lambda)$.
              \end{itemize}
    \end{itemize}

\end{multicols*}
\end{document}
