% Geometria documento.
\documentclass{article}
\usepackage[a4paper, landscape, left=0.5cm, right=0.5cm, top=0.5cm, bottom=0.5cm]{geometry}
% Allineamento contenuti.
\usepackage[document]{ragged2e}
\usepackage{multicol}
% Rimozione spazi verticali liste.
\usepackage{enumitem}
\setlist[itemize]{nosep}
% Utility per formule.
\usepackage{mathtools, nccmath}
\usepackage{amssymb, amsmath}


\begin{document}
% Rimuovo la numerazione delle pagine.
\thispagestyle{empty}
% Ambiente multicolonna su tutto il documento.
\begin{multicols*}{3}

\textbf{\underline{Probabilità}}
\begin{itemize}
    \item \textbf{Teo. Prob. Totali}: $P(\bigcup_{i=1}^{n}P(A_{i}))=\sum_{i=1}^{n}P(A_{i})$; $A_{1},...,A_{n}$ eventi indipendenti.
    \item \textbf{Prob. Condizionate}: $P(A|B)={P(A\cap B)}/{P(B)}$.
    \item \textbf{Teo. Prob. Totali (Partizioni)}: $P(A)=\sum_{i=1}^{n}P(B_{i})P(A|B_{i})$, $B_{1},...,B_{n}$ partizioni di $\Omega$.
    \item \textbf{Teo. Bayes}: $P(A_{i}|B)=P(B|A_{i})P(A_{i})/(\sum_{j=1}^{n}P(B|A_{j})P(A_j))=P(B)$, $A_{i}\in A_{1},...,A_{n}$ partizioni di $\Omega$.
    \item \textbf{Note sui Complementari}: $P(A)=1-P(A^{C})$; $P(A|B)=1-P(A^{C}|B)$.
    \item \textbf{Indipendenza}: $A_{1},...,A_{n}$ eventi indipendenti sse $P(\bigcap_{i=1}^{n}P(A_{i}))=\prod_{i=1}^{n}P(A_{i})$.
    \item \textbf{Note sull'Indipendenza}: $P(A|B)=P(A)$ sse $A, B$ eventi indipendenti; indipendenza condizionata $\nleftrightarrow$ indipendenza incondizionata.
\end{itemize}

\textbf{\underline{Calcolo Combinatorio}}
\begin{itemize}
    \item \textbf{Permutazioni}: $n$ elementi distinti $\rightarrow$ $n!$ permutazioni; se $m\leq n$ elementi sono indistinguibili $\rightarrow$ ${n!}/{m!}$ disposizioni.
    \item \textbf{Sottoinsiemi di $k$ elementi}: $n$ elementi, assunto $k\leq n$ $\rightarrow$ ${n!}/{(n-k)!}$ sottoinsiemi; se l'ordine degli elementi all'interno dei sottoinsiemi non mi importa avrò $\binom{n}{k}={n!}/{k!(n-k)!}$ sottoinsiemi.
    \item \textbf{Prob. Binomiale}: dati $n$ tentativi, $P(succ.)=p$, assunto $k\leq n$ $\rightarrow$ $P({k\; succ.}/{n\; tentativi})=\binom{n}{k}p^{k}(1-p)^{n-k}$.
    \item \textbf{Partizioni}: dato $\Omega$ partizionato in $K_{n}$ sottoinsiemi, calcolo la probabilità, su $n$ tentativi, di ottenere $k_{1}\in K_{1},...,k_{n}\in K_{n}$ elementi $\rightarrow$ $\binom{n}{k_{1},...,k_{n}}={n!}/{k_{1}!k_{2}!...k_{n}}$.
    \item \textbf{Prob. Ipergeometrica}: dato $|\Omega|=n$ partizionato in $K_{1}=k\; K_{2}=n-k$ sottoinsiemi e scelto un campione di $c<n$ elementi voglio calcolare la probabilità che questo sia composto da $k^{'}\leq k\in K_{1}$ e $k^{''} = c-k^{'}\leq n-k\in K_{2}$ elementi $\rightarrow$ ${\binom{k}{k^{'}}\binom{n-k}{c-k^{'}}}/{\binom{n}{c}}$.
\end{itemize}

\textbf{\underline{Variabili Aleatorie Discrete}}
\begin{itemize}
    \item \textbf{Legge di Prob.}: $p_{X}(x)=P(X=x)$.
    \item \textbf{Valore Atteso}: $E[X]=\sum_{x}x\cdot p_{X}(x)$; $E[X|A]=\sum_{x}x\cdot p_{X|A}(x)$ se condizionato.
    \item \textbf{Legge dello Statistico Inconsapevole}: sia $g(x)$ deterministica $\rightarrow$ $E[g(X)]=\sum_{x}g(x)\cdot p_{X}(x)$.
    \item \textbf{Linearità del Valore Atteso}: $E[\alpha X+\beta]=\alpha E[X]+\beta$; se $g(x)$ è determinista e lineare $E[g(X)]=g(E[X])$.
    \item \textbf{Varianza}: $Var[X]=E[X^{2}]-E[X]^{2}=E[(X-E[X])^{2}]$.
    \item \textbf{Semi-Linearità della Varianza}: $Var[\alpha X+\beta]=\alpha^{2}Var[X]$.
    \item \textbf{Deviazione Standard}: $\sigma_{X}=\sqrt{Var[X]}$.
    \item \textbf{Perdita di Memoria}: $p_{X-t|X>t}(x)=p_{X}(x)$.
    \item \textbf{Legge dell'Aspettativa Totale}: $E[X]=\sum_{i=1}^{n}P(A_{i})E(X|A_{i})$, $A_{1},...,A_{n}$ eventi che partizionano $\Omega$.
\end{itemize}

\textbf{\underline{Variabili Aleatorie Discrete Multiple}}
\begin{itemize}
    \item \textbf{Marginalizzazione}: $p_{Y}(y)=\sum_{x}p_{X,Y}(x,y)=\sum_{x}p_{X}(x)\cdot p_{Y|X}(y|x)$.
    \item \textbf{Valore Atteso}: data $g(x,y)$ deterministica $E[g(X,Y)]=\sum_{x}\sum_{y}g(x,y)\cdot p_{X,Y}(x,y)$; caso particolare $E[X+Y]=E[X]+E[Y]$.
    \item \textbf{Varianza}: $Var[X+Y]=Var[X]+Var[Y]+2(E[XY]-E[X]E[Y])$.
    \item \textbf{Casi Particolari (X, Y Indipendenti)}:
    \useshortskip \begin{equation*}
        X\perp Y\Rightarrow \begin{cases}E[XY]=E[X]E[Y]\\ E[g(X)h(Y)]=E[g(X)]E[h(Y)]\\Var[\alpha X+\beta Y]=\alpha^{2}Var[X]+\beta^{2}Var[Y]\end{cases}
    \end{equation*}
\end{itemize}

\textbf{\underline{Variabili Aleatorie Continue}}
\begin{itemize}
    \item \textbf{Densità di Prob.}: $P(x\leq X\leq x+\delta)=\int_{x}^{x+\delta}f_{X}(\gamma)d\gamma$.
    \item \textbf{Valore Atteso}: $E[X]=\int_{-\infty}^{\infty}xf_{X}(x)dx$.
    \item \textbf{Legge dello Statistico Inconsapevole}: $E[g(X)]=\int_{-\infty}^{\infty}g(x)f_{X}(x)dx$.
    \item \textbf{Varianza}: $\sigma^{2}=Var[X]=E[X^{2}]-E[X]^{2}=\int_{-\infty}^{\infty}(x-E[X])^{2}\cdot f_{X}(x)dx$.
    \item \textbf{Cumulata di Prob.}: $F_{X}(x)=P(X\leq x)=\int_{-\infty}^{x}f_{X}(\gamma)d\gamma$.
\end{itemize}

\textbf{\underline{Variabili Aleatorie Continue Multiple}}
\begin{itemize}
    \item \textbf{Marginalizzazione}: $f_{Y}(y)=\int_{x}f_{X,Y}(x,y)dx=\int_{y}f_{Y|X}(y|x)f_{X}(x)dx$.
    \item \textbf{Valore Atteso}: data $g(x,y)$ deterministica $E[g(X,Y)]=\iint_{\mathbb{R}}g(X,Y)f_{X,Y}(x,y)dxdy$.
    \item \textbf{Casi Particolari (X, Y Indipendenti)}: $X\perp Y\Rightarrow f_{x,Y}(x,y)=f_{X}(x)f_{Y}(y)\wedge F_{X,Y}(x,y)=F_{X}(x)F_{Y}(y)$.
    \item \textbf{Teo. Bayes nel Continuo e Situazioni Ibride}:
    \begin{itemize}
        \item $X,Y$ continue: $f_{X|Y}(x,y)=f_{Y|X}(y|x)f_{X}(x)/f_{Y}(y)$.
        \item $X$ discreta, $Y$ continua: $p_{X|Y}(x|y)=f_{Y|X}(y|x)p_{X}(x)/f_{Y}(y)$.
        \item $X$ continua, $Y$ discreta: $f_{X|Y}(x|y)=p_{Y|X}(y|x)f_{X}(x)/p_{Y}(y)$.
    \end{itemize}
    \item \textbf{Somma di Variabili Aleatorie Continue}: Sia $Z=X+Y$:
    \begin{itemize}
        \item $X\perp Y\Longleftrightarrow f_{Z,X}(z,x)=f_X(x)f_Y(z-x)$.
        \item $f_Z(z)=\int_{-\infty}^{+\infty}f_X(x)f_Y(z-x)dx$. (Convoluzione.)
        \item $F_Z(z)=F_{X+Y}(z)=\int_{-\infty}^{+\infty}F_x(z-y)f_Y(y)dy$.
    \end{itemize}
\end{itemize}

\textbf{\underline{Altri Indicatori Statistici per V.A. Multiple}}
\begin{itemize}
    \item \textbf{Covarianza}
    \begin{itemize}
        \item $Cov[X,Y]=E[XY]-E[X]E[Y]$.
        \item $Cov[X,X]=Var[X]$; $Cov[\alpha X,Y]=\alpha Cov[X,Y]$.
        \item $X\perp Y\Rightarrow Cov[X,Y]=0$ ma $Cov[X,Y]=0 \nRightarrow X\perp Y$.
        \item $Cov[\sum_{i=1}^{n}X_{i},\sum_{j=1}^{n}Y_{j}]=\sum_{i=1}^{n}\sum_{j=1}^{n}Cov[X_{i},Y_{j}]$.
        \item $Var[\sum_{i=1}^{n}X_{i}]=\sum_{i=1}^{n}Var[X_{i}]+2\sum \sum_{i<j}Cov[X_{i},X_{j}]$.
    \end{itemize}
    \item \textbf{Coeff. Correlazione Lineare}: $\rho =Cov[X,Y]/\sigma_{X}\sigma_{Y}$. Se $\rho=1\implies (X-E[X])=\alpha(Y-E[Y])$.
    \item \textbf{Valore Atteso Condizionato}: $E[X|Y]=\sum_{x}x\cdot p_{X|Y}(x|y)$.
    \item \textbf{Legge delle Aspettazioni Iterate}: $E[X]=E[E[X|Y]]$.
    \item \textbf{Varianza Condizionata}: $Var[X|Y]=E[X^{2}|Y=y]-E[X|Y=y]^{2}$.
    \item \textbf{Legge della Variazione Totale}: $Var[X]=E[Var[X|Y]]+Var[E[X|Y]]$.
\end{itemize}

\textbf{\underline{Successioni di V.A.}}
\begin{itemize}
    \item \textbf{Somma di $N$ (casuale) V.A. Indipendenti $X_{i}$}:
    \begin{itemize}
        \item $E[\sum_{i=1}^{N}X_{i}]=E[E[\sum_{i=1}^{N}X_{i}N]]=E[N]E[X]$.
        \item $Var[X]=E[N]Var[X_{1}]+Var[N]E[X_{1}]^{2}$.
    \end{itemize}
    \item \textbf{Diseguaglianza di Markov}: $E[X]\geq \alpha P(X\geq \alpha)$.
    \item \textbf{Diseguaglianza di Chebyshev}: $Var[X]\geq \alpha^{2} P(|X-E[X]|\geq \alpha)$.
    \item \textbf{Convergenza in Prob.}: Sia ${A_{k}}$ una successione di V.A. e sia $\alpha\in\mathbb{R}$; ${A_{k}}$ si dice convergente in probabilità ad $\alpha$ se: $\lim_{k\rightarrow +\infty}P(|A_{k}-a|\geq\epsilon)=0\;\forall\epsilon >0$. $A_{k}\rightarrow^{P}\alpha$.
    \item \textbf{Media Campionaria}: Siano $X_{1},X_{2},..,X_{n}$ V.A. I.I.D.; $M_{n}=(X_{1}+X_{2}+..+X_{n})/n$. $M_{n}\rightarrow^{P} E[X]$; $E[M_{n}]=^{n\rightarrow +\infty}E[X]$; $Var[M_{n}]=^{n\rightarrow +\infty}0$.
\end{itemize}

\textbf{\underline{V.A. Notevoli}}
\begin{itemize}
    \item \textbf{V.A. Uniforme $X\sim Unif[\alpha, \beta]$}:
    \begin{itemize}
        \item $f_{X}(x)=\begin{cases}
            1/(\beta-\alpha) &\alpha<x<\beta \\
            0 &altr.
        \end{cases}$
        \item $F_{X}(x)=\begin{cases}
            0 &x<\alpha \\
            (x-\alpha)/(\beta-\alpha) &\alpha<x<\beta \\
            1 &x>\beta
        \end{cases}$
        \item $E[X]=\frac{1}{2}(\alpha +\beta)$; $Var[X]=\frac{1}{12}(\beta -\alpha)^{2}$.
        \item Se discretizzata: $Var[X]=\frac{n^{2}-1}{12}$.
    \end{itemize}
    \newpage
    \item \textbf{V.A. Geometriche $X\sim Geom(p)$}
    \begin{itemize}
        \item $p_{X}(x)=(1-p)^{x-1}p$.
        \item $E[X]=1/p$; $Var[X]=(1-p)/p^{2}$.
    \end{itemize}
    \item \textbf{V.A. Binomiali/di Bernoulli $X\sim Bin(n,p)$} (se $n=1$ la binomiale torna bernoulliana).
    \begin{itemize}
        \item $p_{X_{i}}(1/succ.)=p$; $p_{X_{i}}(0/insucc.)=1-p$.
        \item $p_{B}(i)=p_{X_{1}+X_{2}+...+X_{n}}(i)=\binom{n}{i}p^{i}(1-p)^{n-i}$.
        \item $E[X]=np$; $Var[X]=np(1-p)$.
    \end{itemize}
    \item \textbf{V.A. Esponenziale $X\sim Exp(\lambda)$}.
    \begin{itemize}
        \item $f_{X}(x)=\begin{cases}
            \lambda e^{-x\lambda} &x\geq 0 \\
            0 &x<0
        \end{cases}$.
        \item $F_{X}(x)=1-e^{-x\lambda}$.
        \item $E[X]=1/\lambda$; $Var[X]=1/\lambda^{2}$.
    \end{itemize}
    \item \textbf{V.A. Gaussiane $X\sim\mathcal{N}(\mu,\sigma^{2})$}.
    \begin{itemize}
        \item $f_{X}(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}$.
        \item $E[X]=\mu$; $Var[X]=\sigma^{2}$.
        \item $\mathcal{N}(0,1)$ è la gaussiana standard i cui valori di cumulata sono tabulati. $X\sim\mathcal{N}(\mu, \sigma^{2})\implies \frac{X-\mu}{\sigma}\sim\mathcal{N}(0,1)$.
        \item $Y=\alpha X+\beta\implies Y\sim\mathcal{N}(\alpha\mu +\beta,\alpha^{2}\sigma^{2})$.
        \item $X\perp Y$; $X\sim\mathcal{N}(\mu_{'}, \sigma_{'}^{2})$, $Y\sim\mathcal{N}(\mu_{''}, \sigma_{''}^{2})$; $Z=X+Y\sim\mathcal{N}(\mu_{'}+\mu_{''}, \sigma_{'}^{2}+\sigma_{''}^{2})$. In ogni caso, anche se $X\not\perp Y$, $E[Z]=E[X]+E[Y]$; la varianza, invece, necessita del fattore correttivo. La somma di gaussiane sarà sempre gaussiana.
    \end{itemize}
\end{itemize}
\textbf{\underline{Calcolo di una Funzione di V.A.}}
\begin{itemize}
    \item Discr. $p_{Y}(y)=P(g(X)=y)=\sum_{x|g(x)=y}p_{X}(x)$.
    \item Cont. (Cumulata) $F_{Y}(y)=P(g(X)\leq y)\implies f_{Y}(y)=dF_{y}(y)/dy$.
    \item Cont. ($g(X)$ Lineare) $Y=\alpha X+\beta\implies f_{Y}(y)=\frac{1}{|\alpha|}f_{X}(\frac{x-\beta}{\alpha})$.
    \item Cont. ($g(X)$ Monotona) $Y=g(X)\implies f_{Y}(y)=f_{X}(g^{-1}(y))/\left|\frac{d}{dx}g(g^{-1}(y))\right|$.
\end{itemize}

\end{multicols*}
\end{document}
